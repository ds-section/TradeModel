{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import fetcher as ft\n",
    "import slreg as slr\n",
    "\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def process_string(s):\n",
    "    processed = (s.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r'[\\t\\n\\r\\f\\v]', r'')\n",
    "                  .str.replace(r'\\d+', r'')\n",
    "                  # capture commas followed by any number of whitespaces\n",
    "                  .str.replace(r', *', r' ')\n",
    "                  .apply(lambda s: s.translate(str.maketrans({x: None for x in string.punctuation}))\n",
    "                         if type(s) == str else '')\n",
    "                  # apply SnowballStemmer then WordNetLemmatizer to singularize missed words\n",
    "                  .apply(lambda s: ' '.join(set([wnl.lemmatize(sno.stem(x)) for x in re.split(r' +', s)\n",
    "                                                 if x not in stopwords.words('english')]))\n",
    "                         if type(s) == str else ''))\n",
    "    return processed\n",
    "\n",
    "\n",
    "def n_common_terms(string, term_list):\n",
    "    return len(set(string.split(' ')) & set(term_list))\n",
    "\n",
    "\n",
    "def compatibility(s, term_list, func='mean'):\n",
    "    func={'mean': pd.Series.mean,\n",
    "          'max': pd.Series.max,\n",
    "          'min': pd.Series.min}[func]\n",
    "    comp = (s.str.split('|')\n",
    "            .apply(lambda ls:\n",
    "                   pd.Series(map(lambda x: n_common_terms(x, term_list), ls))))\n",
    "    comp = func(comp, axis=1)\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inqs = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/05. TAITRA/FCRM/data/inquiry_compressed_with_country.csv',\n",
    "                   parse_dates=True, index_col='creation_date', encoding='utf-8',\n",
    "                   dtype={'code_val': str, 'dept': 'category'})\n",
    "ctlg = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/05. TAITRA/TT/processed_ctlg.csv',\n",
    "                   index_col='ban', parse_dates=['mod_date'], dtype={'code_val': str})\n",
    "ex = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/05. TAITRA/TT/export_compressed.csv',\n",
    "                 index_col='ban', dtype={'ban': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_features(inq):\n",
    "    \"\"\"Return DataFrame of calculated features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inq : Series\n",
    "        A single buyer inquiry.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : DataFrame\n",
    "        Currently there are 28 features:\n",
    "        \n",
    "        1) ``n_items`` : number of items in the supplier's catalogue.\n",
    "        \n",
    "        2) ``{type_}comp_{combination}`` : product compatibility. There are three types (total, max,\n",
    "           min) and eight possible combinations.\n",
    "        \n",
    "        3) ``recency`` : date difference between last modified date and today.\n",
    "        \n",
    "        4) ``n_comms`` : number of unique commodities exported by the supplier (with non-empty\n",
    "           description).\n",
    "        \n",
    "        5) ``isexporter`` : whether the supplier has shipped to buyer's country in recent years.\n",
    "    \"\"\"\n",
    "    \n",
    "    code, prod, desc, ctry = inq['code_val'], inq['prod_name'], inq['prod_desc'], inq['buyer_country']\n",
    "    prod_ls = process_string(pd.Series(prod))[0].split(' ')\n",
    "    desc_ls = process_string(pd.Series(desc))[0].split(' ')\n",
    "    supp = ft.fetch_suppliers(ctlg, code)\n",
    "    bans = supp.index\n",
    "    export = ft.fetch_export(ex, bans, ctry)\n",
    "    supp = pd.concat([supp, export], axis=1)\n",
    "    \n",
    "    comp_ls = [lambda c: compatibility(c, prod_ls, 'mean'),\n",
    "               lambda c: compatibility(c, prod_ls, 'max'),\n",
    "               lambda c: compatibility(c, prod_ls, 'min'),\n",
    "               lambda c: compatibility(c, desc_ls, 'mean'),\n",
    "               lambda c: compatibility(c, desc_ls, 'max'),\n",
    "               lambda c: compatibility(c, desc_ls, 'min')]\n",
    "    \n",
    "    features = supp.transform(dict(\n",
    "        n_items=lambda c: c,\n",
    "        item_name=comp_ls,\n",
    "        item_desc=comp_ls,\n",
    "        keyword=comp_ls,\n",
    "        recency=lambda c: c,\n",
    "        n_comms=lambda c: c,\n",
    "        comm_name=comp_ls,\n",
    "        isexporter=lambda c: c\n",
    "        )\n",
    "    )\n",
    "\n",
    "    features = features.swaplevel(axis=1)\n",
    "    features.columns = features.columns.droplevel()\n",
    "    features.columns = ['n_items',\n",
    "                        'comp_ip', 'max_comp_ip', 'min_comp_ip',  # item vs prod\n",
    "                        'comp_id', 'max_comp_id', 'min_comp_id',  # item vs desc\n",
    "                        'comp_dp', 'max_comp_dp', 'min_comp_dp',  # desc vs prod\n",
    "                        'comp_dd', 'max_comp_dd', 'min_comp_dd',  # desc vs desc\n",
    "                        'comp_kp', 'max_comp_kp', 'min_comp_kp',  # keyword vs prod\n",
    "                        'comp_kd', 'max_comp_kd', 'min_comp_kd',  # keyword vs desc\n",
    "                        'recency', 'n_comms',\n",
    "                        'comp_hp', 'max_comp_hp', 'min_comp_hp',  # hs vs prod\n",
    "                        'comp_hd', 'max_comp_hd', 'min_comp_hd',  # hs vs desc\n",
    "                        'isexporter']\n",
    "    return features\n",
    "\n",
    "\n",
    "def estimate_feature_dist(inqs):\n",
    "    \"\"\"Estimate and save population mean and standard deviation for each feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inqs : DataFrame\n",
    "        Each row represents an inquiry.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_map = map(compute_features, [row for lab, row in inqs.iterrows()])\n",
    "    stacked = reduce(lambda x, y: x.append(y), feature_map)\n",
    "    mean, std = stacked.mean(), stacked.std()\n",
    "    dist = pd.concat([mean, std], axis=1)\n",
    "    dist.columns = ['mean', 'std']\n",
    "    dist.to_csv('feature_distribution.csv')\n",
    "    return\n",
    "\n",
    "\n",
    "def normalize_features(X):\n",
    "    \"\"\"Return normalized features.\"\"\"\n",
    "    return (X - mean) / std\n",
    "\n",
    "\n",
    "# Some useful values\n",
    "n = 14\n",
    "alpha = 0.01\n",
    "dist = pd.read_csv('feature_distribution.csv', index_col=0)\n",
    "mean, std = dist['mean'], dist['std']\n",
    "\n",
    "# For each incoming inquiry Series ``inq``, run:\n",
    "# ================================================================\n",
    "# Load current theta or initialize if not exists\n",
    "if len(glob('theta.txt')):\n",
    "    with open('theta.txt', 'r') as f:\n",
    "        theta = np.array([float(x) for x in f.read().split('\\n')])\n",
    "else:\n",
    "    theta = np.zeros(n + 1)\n",
    "\n",
    "# Get data ready\n",
    "X = compute_features(inq)\n",
    "X = normalize_features(X)\n",
    "X['intercept'] = 1\n",
    "\n",
    "# Predict probabilities\n",
    "X['prob'] = slr.predict_prob(X, theta)\n",
    "\n",
    "# Get top 10 suppliers\n",
    "top10 = X.sort_values('prob', ascending=False).drop('prob', axis=1).head(10)\n",
    "\n",
    "# Fetch user response Series ``y``\n",
    "\n",
    "# Save y together with ``inq`` (broadcasted) and 10 ``BAN_REAL``s\n",
    "\n",
    "# Update theta using 10 steps of gradient descent\n",
    "for i in range(10):\n",
    "    x = top10.iloc[i].values.reshape((1, n))\n",
    "    theta, J = slr.gradient_descent(x, y[[i]], theta, alpha)\n",
    "\n",
    "with open('theta.txt', 'w') as f:\n",
    "    f.write('\\n'.join([str(x) for x in theta]))\n",
    "# ================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
