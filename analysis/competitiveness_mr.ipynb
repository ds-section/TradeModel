{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitiveness Analysis (by Country and Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce version to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "src = 'un'\n",
    "\n",
    "if src == 'itc':\n",
    "    path = '//172.20.23.190/ds/Raw Data/2016大數爬蟲案/data/ITC HS6/all/'\n",
    "    files = pd.Series(os.listdir(path))\n",
    "    # Filter for import data\n",
    "    files = files[files.str.contains('_I')]\n",
    "    # Exclude Taiwan and world total from importing countries\n",
    "    files.drop(files[files.str.contains('Taipei|All_')].index.values, inplace=True)\n",
    "else:\n",
    "    path = '//172.26.1.102/dstore/uncomtrade/annual_reduced/'\n",
    "    files = ['un-import-hs6-shiny-2012-2015.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load product description file\n",
    "desc = pd.read_csv('//172.26.1.102/dstore/Projects/mof-crawler/full_hscode11.tsv', sep='\\t',\n",
    "                   dtype='str', usecols=['hs2cn', 'hs4cn', 'hs6', 'hs6cn'])\n",
    "desc.columns = ['desc2', 'desc4', 'product', 'desc6']\n",
    "# Because each row corresponds to an HS11 code in the original table, need to remove duplicates\n",
    "desc.drop_duplicates(subset='product', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Country name mapping table\n",
    "ctry_map = pd.read_csv('//172.20.23.190/ds/Raw Data/2016大數爬蟲案/data/ITC HS6/itc_df_complete.csv',\n",
    "                       usecols=[dict(itc='itc_name', un='ds_code')[src], 'countryName'],\n",
    "                       dtype=dict(itc=None, un={'ds_code': str})[src])\n",
    "ctry_map.columns = ['country', 'ch_name']\n",
    "# Convert to en -> zh dictionary (UN: ISO 3166-1 numeric code -> zh)\n",
    "ctry_map = ctry_map.set_index('country').to_dict()['ch_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Company export data\n",
    "com_ex = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/06. companies/財政部廠商進出口資料/KMG_HS6COUNTRY.csv',\n",
    "                     names=['ban', 'code', 'country', 'year', 'month', 'ex', 'im'], header=0,\n",
    "                     dtype={'ban': str, 'code': str, 'country': str, 'year': str, 'month': str,\n",
    "                            'ex': int, 'im': int})\n",
    "com_ex['code'] = com_ex['code'].str.zfill(6)\n",
    "# Remove yearly total rows\n",
    "com_ex = com_ex[com_ex['month'].notnull()]\n",
    "# Pad zeros and construct DatetimeIndex\n",
    "com_ex['month'] = com_ex['month'].apply(lambda x: x.zfill(2))\n",
    "com_ex.index = pd.to_datetime(com_ex['year'] + com_ex['month'], format='%Y%m')\n",
    "com_ex.index.name = 'date'\n",
    "# Drop original year and month columns\n",
    "com_ex = com_ex.drop(['year', 'month'], axis=1)\n",
    "\n",
    "# Extract 2015 data and sum for yearly total\n",
    "com_ex = com_ex['2015'].groupby(['ban', 'code', 'country']).sum().reset_index()\n",
    "# Filter out zero entries\n",
    "com_ex = com_ex[com_ex['ex'] != 0]\n",
    "# Alternatively, for each commodity, keep only companies with over 100 thousand USD worth of\n",
    "# yearly export\n",
    "# sizable = com_ex.groupby(['ban', 'code']).sum().reset_index().query('ex != 0')['ban'].unique()\n",
    "# com_ex = com_ex[com_ex['ban'].isin(sizable)]\n",
    "\n",
    "# Company profile datasets\n",
    "path_crm = 'C:/Users/2093/Desktop/Data Center/03. Data/05. TAITRA/CRM/'\n",
    "tax = pd.read_csv(path_crm + 'tax_utf-8.csv', header=0, index_col=False,\n",
    "                  names=['bogus', 'address', 'id', 'com_name', 'capital', 'est_date', 'invoice',\n",
    "                         'biz1_id', 'biz1', 'biz2_id', 'biz2', 'biz3_id', 'biz3', 'biz4_id', 'biz4'],\n",
    "                  dtype={'bogus': str,\n",
    "                         'address': str,\n",
    "                         'id': str,\n",
    "                         'com_name': str,\n",
    "                         'capital': int,\n",
    "                         'est_date': int,\n",
    "                         'invoice': str,\n",
    "                         'biz1_id': str,\n",
    "                         'biz1': str,\n",
    "                         'biz2_id': str,\n",
    "                         'biz2': str,\n",
    "                         'biz3_id': str,\n",
    "                         'biz3': str,\n",
    "                         'biz4_id': str,\n",
    "                         'biz4': str})\n",
    "biz_nature = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/06. companies/COMP_TYPE.csv',\n",
    "                         index_col=False, header=0,\n",
    "                         names=['biz_nature', 'biz_nature_id', 'id', 'user', 'date', 'bogus'],\n",
    "                         dtype={'biz_nature': str,\n",
    "                                'biz_nature_id': int,\n",
    "                                'id': str,\n",
    "                                'user': str,\n",
    "                                'date': str,\n",
    "                                'bogus': str})\n",
    "# List of Taiwan exporters\n",
    "exporter = biz_nature.loc[biz_nature['biz_nature'] == 'Exporter', 'id']\n",
    "# Event participation data\n",
    "event = pd.read_csv(path_crm + 'crm_group.csv', usecols=range(4), header=0,\n",
    "                    names=['year', 'event_name', 'source', 'event_id'])\n",
    "attend = pd.read_csv(path_crm + 'crm_basic_group.csv', usecols=range(2), header=0,\n",
    "                     names=['event_id', 'bogus'])\n",
    "# Companies that have participated in TAITRA event at least once within 2013-2016\n",
    "participant = attend.merge(event[['year', 'event_id']], on='event_id').merge(\n",
    "    tax[['bogus', 'id']], on='bogus').query('year > 2013')['id'].unique()\n",
    "\n",
    "# ISO 3166-1 Alpha-2 to zh name mapping\n",
    "alpha_map = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/03. import_export/ds_export_market_framework.csv',\n",
    "                        usecols=['mof_enCode', 'export_countryName'])\n",
    "alpha_map.columns = ['alpha2', 'ch_name']\n",
    "# Convert to Alpha-2 -> zh dictionary\n",
    "alpha_map = alpha_map.set_index('alpha2').to_dict()['ch_name']\n",
    "\n",
    "# Alternative: ISO 3166-1 Alpha-2 to numeric code mapping\n",
    "# alpha_map = pd.read_csv('C:/Users/2093/Desktop/Data Center/03. Data/03. import_export/ds_export_market_framework.csv',\n",
    "#                         usecols=['mof_enCode', 'iso_numCode'], dtype=str)\n",
    "# alpha_map.columns = ['alpha2', 'num']\n",
    "# alpha_map['num'] = alpha_map['num'].str.replace('^0+', '')\n",
    "# alpha_map = alpha_map.set_index('alpha2').to_dict()['num']\n",
    "\n",
    "# For each country and commodity, compute no. of exporters and among them, those that have\n",
    "# participated in TRATRA events\n",
    "n_com = com_ex[com_ex['ban'].isin(exporter)].assign(\n",
    "    is_par = lambda x: x['ban'].isin(participant)).groupby(['country', 'code']).agg(\n",
    "    {'ban': 'count', 'is_par': sum}).reset_index().rename(\n",
    "    columns=dict(code='product', ban='n_exporter'))\n",
    "n_com['country'].replace(alpha_map, inplace=True)\n",
    "n_com['is_par'] = n_com['is_par'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def aggr_data(file):\n",
    "    \n",
    "    if src == 'itc':\n",
    "        df = pd.read_csv(path + file, index_col=0,\n",
    "                         dtype={'Country': 'object',\n",
    "                                'Product Code': 'object',\n",
    "                                'Partner': 'object',\n",
    "                                'Value in 2001': 'float',\n",
    "                                'Value in 2002': 'float',\n",
    "                                'Value in 2003': 'float',\n",
    "                                'Value in 2004': 'float',\n",
    "                                'Value in 2005': 'float',\n",
    "                                'Value in 2006': 'float',\n",
    "                                'Value in 2007': 'float',\n",
    "                                'Value in 2008': 'float',\n",
    "                                'Value in 2009': 'float',\n",
    "                                'Value in 2010': 'float',\n",
    "                                'Value in 2011': 'float',\n",
    "                                'Value in 2012': 'float',\n",
    "                                'Value in 2013': 'float',\n",
    "                                'Value in 2014': 'float',\n",
    "                                'Value in 2015': 'float'}).reset_index(drop=True)\n",
    "    else:\n",
    "        df = pd.read_csv(path + file,\n",
    "                         dtype={'Country': 'object',\n",
    "                                'Product Code': 'object',\n",
    "                                'Partner': 'object',\n",
    "                                'Value in 2012': 'float',\n",
    "                                'Value in 2013': 'float',\n",
    "                                'Value in 2014': 'float',\n",
    "                                'Value in 2015': 'float'}).loc[\n",
    "            :, ['Country', 'Product Code', 'Partner',\n",
    "                'Value in 2012', 'Value in 2013', 'Value in 2014', 'Value in 2015']]\n",
    "\n",
    "    # Remove the leading single quote (') in product code column\n",
    "    df['Product Code'] = df['Product Code'].apply(lambda x: x[1:])\n",
    "    # If UN, exclude Taiwan ('490') and world total ('all') from importing countries\n",
    "    df = df[~df['Country'].str.contains('490|all')]\n",
    "    # Remove rows for commodities sum\n",
    "    df = df[df['Product Code'] != 'TOTAL']\n",
    "    # Remove rows where partner is 'All' (it seems that HS6 tables don't have this code)\n",
    "    # In UN's case, this becomes 'all' and '0' (the latter stands for World)\n",
    "    df = df[~df['Partner'].str.contains('All|all|^0$')]\n",
    "    # Select only columns for 2012 to 2015\n",
    "    df = pd.concat((df.loc[:, :'Partner'], df.loc[:, 'Value in 2012':]), axis=1)\n",
    "    df.columns = ['country', 'product', 'partner', 'val12', 'val13', 'val14', 'val15']\n",
    "    # Compute growth rates\n",
    "    def growthRate(data, start_year, end_year):\n",
    "        return ((data['val' + str(end_year)] - data['val' + str(start_year)])\n",
    "                / data['val' + str(start_year)] * 100)\n",
    "    df['g13'] = growthRate(df, 12, 13)\n",
    "    df['g14'] = growthRate(df, 13, 14)\n",
    "    df['g15'] = growthRate(df, 14, 15)\n",
    "\n",
    "    # Compute total imports for all (country, product) pairs\n",
    "    total = df.groupby(['country', 'product']).agg({\n",
    "            'val12': 'sum',\n",
    "            'val13': 'sum',\n",
    "            'val14': 'sum',\n",
    "            'val15': 'sum'})\n",
    "    total['g13'] = growthRate(total, 12, 13)\n",
    "    total['g14'] = growthRate(total, 13, 14)\n",
    "    total['g15'] = growthRate(total, 14, 15)\n",
    "    total = total[['val15', 'g13', 'g14', 'g15']].reset_index()\n",
    "\n",
    "    # Compute commodity-wise market share for each partner country\n",
    "    df['share'] = df['val15'] / df.groupby(['country', 'product'])['val15'].transform('sum') * 100\n",
    "    # Compute commodity-wise rank for each partner country\n",
    "    df['rank'] = df.groupby(['country', 'product'])['val15'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Compute no. of non-zero partners for each importing country by commodity\n",
    "    n_partner = df[(df['val15'] != 0) & (df['val15'].notnull())].groupby(\n",
    "        ['country', 'product']).agg({'partner': 'count'}).rename(columns={'partner': 'n_partner'})\n",
    "    # Compute excess kurtosis for each country by commodity\n",
    "    kurtos = df.groupby(['country', 'product']).agg(\n",
    "        {'val15': lambda x: kurtosis(x, nan_policy='omit')}).rename(columns={'val15': 'kurtos'})\n",
    "    # Compute Pearson's median skewness coefficient for each country by commodity\n",
    "    skewness = df.groupby(['country', 'product']).agg(\n",
    "        {'val15': lambda x: 3 * (x.mean() - x.median()) / x.std() if x.std() != 0 else np.nan}).rename(\n",
    "        columns={'val15': 'skew'})\n",
    "\n",
    "    # Extract data for Taiwan (UN: '490')\n",
    "    tw = df.loc[df['partner'].str.contains('Taipei, Chinese|490'),\n",
    "                ['country', 'product', 'val15', 'g13', 'g14', 'g15', 'share', 'rank']]\n",
    "    tw.columns = ['country', 'product', 'tw_val15', 'tw_g13', 'tw_g14', 'tw_g15',\n",
    "                  'tw_share', 'tw_rank']\n",
    "    # When import value from Taiwan is zero, manually overwrite corresponding rank of Taiwan with NaN\n",
    "    tw.loc[tw['tw_val15'] == 0, 'tw_rank'] = None\n",
    "\n",
    "    # Extract data for top 3\n",
    "    top3 = df.groupby(['country', 'product']).apply(lambda x: x.nsmallest(3, 'rank')).loc[\n",
    "        :, ['country', 'product', 'partner', 'val15', 'g13', 'g14', 'g15', 'share']]\n",
    "    def getCountryByRank(data, rank):\n",
    "        rs = data.groupby(['country', 'product']).nth(rank).reset_index().loc[\n",
    "        :, ['country', 'product', 'partner', 'val15', 'g13', 'g14', 'g15', 'share']]\n",
    "        rs.columns = (['country', 'product', 'partner']\n",
    "                      + [str(rank + 1) + '_' + x for x in ['val15', 'g13', 'g14', 'g15', 'share']])\n",
    "        return rs\n",
    "    first  = getCountryByRank(top3, 0)\n",
    "    second = getCountryByRank(top3, 1)\n",
    "    third  = getCountryByRank(top3, 2)\n",
    "\n",
    "    # Merge all tables\n",
    "    rs = total.merge(n_partner, how='left', left_on=['country', 'product'], right_index=True).merge(\n",
    "        kurtos, how='left', left_on=['country', 'product'], right_index=True).merge(\n",
    "        skewness, how='left', left_on=['country', 'product'], right_index=True).merge(\n",
    "        #n_com, how='left', on=['country', 'product']).merge(\n",
    "        tw, how='left', on=['country', 'product']).merge(\n",
    "        first, how='left', on=['country', 'product']).rename(columns={'partner': '1_name'}).merge(\n",
    "        second, how='left', on=['country', 'product']).rename(columns={'partner': '2_name'}).merge(\n",
    "        third, how='left', on=['country', 'product']).rename(columns={'partner': '3_name'}).merge(\n",
    "        desc, how='left', on='product').iloc[:, [0, 1, -3, -2, -1] + list(range(2, 33))]\n",
    "    \n",
    "    # Replace en country names (UN: ISO 3166-1 numeric code) with zh names\n",
    "    for col in ['country', '1_name', '2_name', '3_name']:\n",
    "        if src == 'itc':\n",
    "            rs[col].replace(ctry_map, inplace=True)\n",
    "        else:\n",
    "            rs[col] = rs[col].str.zfill(3).replace(ctry_map)\n",
    "\n",
    "    # Merge with n_com on zh names\n",
    "    rs = rs.merge(n_com, how='left', on=['country', 'product']).iloc[\n",
    "        :, list(range(12)) + [-2, -1] + list(range(12, 36))]\n",
    "        \n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2093\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:4185: UserWarning: Warning: converting a masked element to nan.\n",
      "  warnings.warn(\"Warning: converting a masked element to nan.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_map = map(lambda f: aggr_data(f), files)\n",
    "df = reduce(lambda x, y: pd.concat([x, y], axis=0, ignore_index=True), df_map)\n",
    "\n",
    "# Output results\n",
    "df.to_csv('comp_aggregate_6.csv', sep=',', index=False)\n",
    "# Output HS code table\n",
    "df[['product', 'desc2', 'desc4', 'desc6']].drop_duplicates().sort_values('product').to_csv(\n",
    "    'hs_table.csv', sep=',', index=False)\n",
    "# Separate tables\n",
    "for hs2 in df['product'].str.slice(0, 2).unique():\n",
    "    df[df['product'].str.contains('^{}'.format(hs2))].to_csv(\n",
    "        'comp_aggregate_6_{}.csv'.format(hs2), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output no. of Taiwan exporters by commodity only\n",
    "n_com_p = com_ex[com_ex['ban'].isin(exporter)].assign(\n",
    "    is_par = lambda x: x['ban'].isin(participant)).drop_duplicates(['ban', 'code']).groupby('code').agg(\n",
    "    {'ban': 'count', 'is_par': sum}).reset_index().rename(\n",
    "    columns=dict(code='product', ban='n_exporter'))\n",
    "n_com_p['par_rate'] = 100 * n_com_p['is_par'] / n_com_p['n_exporter']\n",
    "n_com_p['is_par'] = n_com_p['is_par'].astype(int)\n",
    "n_com_p.to_csv('n_com_p.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extra columns for viz\n",
    "# For each product, flag wheter Taiwan is an exporter\n",
    "df['tw_is_ex'] = df.groupby('product')['tw_val15'].transform(lambda x: x.any())\n",
    "# Bin market share to invervals\n",
    "df['share_int'] = pd.cut(df['tw_share'], bins=list(np.arange(0, 51, 12.5)) + [100])\n",
    "df['share_int'] = df['share_int'].cat.add_categories(['0'])\n",
    "df['share_int'] = df['share_int'].cat.reorder_categories(\n",
    "    [df['share_int'].cat.categories[-1]] + list(df['share_int'].cat.categories[:-1]), ordered=True)\n",
    "df.loc[df['share_int'].isnull(), 'share_int'] = '0'\n",
    "# Log-transform\n",
    "df['log_val15'] = np.log(df['val15'])\n",
    "# For growth rates, we need to handle negative values\n",
    "for c in ['g13', 'g14', 'g15']:\n",
    "    df['log_' + c] = df.groupby('country')[c].transform(lambda x: np.log(x + np.abs(x.min()) + 1))\n",
    "\n",
    "viz_df = df.loc[df['val15'].notnull(), ['country', 'product', 'desc2', 'desc4', 'desc6', 'val15',\n",
    "                                        'g13', 'g14', 'g15', 'tw_val15', 'tw_share', 'tw_is_ex',\n",
    "                                        'share_int', 'log_val15', 'log_g13', 'log_g14', 'log_g15']]\n",
    "countries = sorted(viz_df['country'].unique())\n",
    "pd.Series(countries).to_csv('country_list.csv', index=False)\n",
    "for c in countries:\n",
    "    viz_df[viz_df['country'] == c].drop('country', axis=1).to_csv(\n",
    "        'bycountry_{}.csv'.format(c), index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
