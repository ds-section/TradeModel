{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitiveness Analysis (by Country and Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce version to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "aggr_level = '6'\n",
    "path = '//172.20.23.190/ds/Raw Data/2016大數爬蟲案/data/ITC HS{}/all/'.format(aggr_level)\n",
    "files = pd.Series(os.listdir(path))\n",
    "# Filter for import data\n",
    "files = files[files.str.contains('_I')]\n",
    "# Exclude Taiwan from importing countries\n",
    "files.drop(files[files.str.contains('Taipei')].index.values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load product description file\n",
    "desc = pd.read_csv('//172.26.1.102/dstore/Projects/mof-crawler/full_hscode11.tsv', sep='\\t',\n",
    "                   dtype='str', usecols=['hs2cn', 'hs4cn', 'hs6', 'hs6cn'])\n",
    "desc.columns = ['desc2', 'desc4', 'product', 'desc6']\n",
    "# Because each row corresponds to an HS11 code in the original table, need to remove duplicates\n",
    "desc.drop_duplicates(subset='product', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Country name mapping table\n",
    "ctry_map = pd.read_csv('//172.20.23.190/ds/Raw Data/2016大數爬蟲案/data/ITC HS6/itc_df_complete.csv',\n",
    "                       usecols=['itc_name', 'countryName'])\n",
    "ctry_map.columns = ['country', 'ch_name']\n",
    "# Convert to en -> zh dictionary\n",
    "ctry_map = ctry_map.set_index('country').to_dict()['ch_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def aggr_data(file):\n",
    "    \n",
    "    df = pd.read_csv(path + file, index_col=0,\n",
    "                     dtype={'Country': 'object',\n",
    "                            'Product Code': 'object',\n",
    "                            'Partner': 'object',\n",
    "                            'Value in 2001': 'float',\n",
    "                            'Value in 2002': 'float',\n",
    "                            'Value in 2003': 'float',\n",
    "                            'Value in 2004': 'float',\n",
    "                            'Value in 2005': 'float',\n",
    "                            'Value in 2006': 'float',\n",
    "                            'Value in 2007': 'float',\n",
    "                            'Value in 2008': 'float',\n",
    "                            'Value in 2009': 'float',\n",
    "                            'Value in 2010': 'float',\n",
    "                            'Value in 2011': 'float',\n",
    "                            'Value in 2012': 'float',\n",
    "                            'Value in 2013': 'float',\n",
    "                            'Value in 2014': 'float',\n",
    "                            'Value in 2015': 'float'}).reset_index(drop=True)\n",
    "\n",
    "    # Remove the leading single quote (') in product code column\n",
    "    df['Product Code'] = df['Product Code'].apply(lambda x: x[1:])\n",
    "    # Remove rows for commodities sum\n",
    "    df = df[df['Product Code'] != 'TOTAL']\n",
    "    # Remove rows where partner is 'All' (it seems that HS6 tables don't have this code)\n",
    "    df = df[df['Partner'] != 'All']\n",
    "    # Select only columns for 2012 to 2015\n",
    "    df = pd.concat((df.loc[:, :'Partner'], df.loc[:, 'Value in 2012':]), axis=1)\n",
    "    df.columns = ['country', 'product', 'partner', 'val12', 'val13', 'val14', 'val15']\n",
    "    # Compute growth rates\n",
    "    def growthRate(data, start_year, end_year):\n",
    "        return (data['val' + str(end_year)] - data['val' + str(start_year)]) /\\\n",
    "               data['val' + str(start_year)] * 100\n",
    "    df['g13'] = growthRate(df, 12, 13)\n",
    "    df['g14'] = growthRate(df, 13, 14)\n",
    "    df['g15'] = growthRate(df, 14, 15)\n",
    "\n",
    "    # Compute total imports for all (country, product) pairs\n",
    "    total = df.groupby(['country', 'product']).agg({\n",
    "            'val12': 'sum',\n",
    "            'val13': 'sum',\n",
    "            'val14': 'sum',\n",
    "            'val15': 'sum'})\n",
    "    total['g13'] = growthRate(total, 12, 13)\n",
    "    total['g14'] = growthRate(total, 13, 14)\n",
    "    total['g15'] = growthRate(total, 14, 15)\n",
    "    total = total[['val15', 'g13', 'g14', 'g15']].reset_index()\n",
    "\n",
    "    # Compute commodity-wise market share for each partner country\n",
    "    df['share'] = df['val15'] / df.groupby(['country', 'product'])['val15'].transform('sum') * 100\n",
    "    # Compute commodity-wise rank for each partner country\n",
    "    df['rank'] = df.groupby(['country', 'product'])['val15'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Compute no. of non-zero partners for each importing country by commodity\n",
    "    n_partner = df[(df['val15'] != 0) & (df['val15'].notnull())].groupby(\n",
    "        ['country', 'product']).agg({'partner': 'count'}).rename(columns={'partner': 'n_partner'})\n",
    "    # Compute Pearson's median skewness coefficient for each country by commodity\n",
    "    skewness = df.groupby(['country', 'product']).agg(\n",
    "        {'val15': lambda x: 3 * (x.mean() - x.median()) / x.std() if x.std() != 0 else np.nan}).rename(\n",
    "        columns={'val15': 'skew'})\n",
    "\n",
    "    # Extract data for Taiwan\n",
    "    tw = df.loc[df['partner'] == 'Taipei, Chinese',\n",
    "                ['country', 'product', 'val15', 'g13', 'g14', 'g15', 'share', 'rank']]\n",
    "    tw.columns = ['country', 'product', 'tw_val15', 'tw_g13', 'tw_g14', 'tw_g15',\n",
    "                  'tw_share', 'tw_rank']\n",
    "    # When import value from Taiwan is zero, manually overwrite corresponding rank of Taiwan with NaN\n",
    "    tw.loc[tw['tw_val15'] == 0, 'tw_rank'] = None\n",
    "\n",
    "    # Extract data for top 3\n",
    "    top3 = df.groupby(['country', 'product']).apply(lambda x: x.nsmallest(3, 'rank')).loc[\n",
    "        :, ['country', 'product', 'partner', 'val15', 'g13', 'g14', 'g15', 'share']]\n",
    "    def getCountryByRank(data, rank):\n",
    "        rs = data.groupby(['country', 'product']).nth(rank).reset_index().loc[\n",
    "        :, ['country', 'product', 'partner', 'val15', 'g13', 'g14', 'g15', 'share']]\n",
    "        rs.columns = ['country', 'product', 'partner'] +\\\n",
    "        [str(rank + 1) + '_' + x for x in ['val15', 'g13', 'g14', 'g15', 'share']]\n",
    "        return rs\n",
    "    first  = getCountryByRank(top3, 0)\n",
    "    second = getCountryByRank(top3, 1)\n",
    "    third  = getCountryByRank(top3, 2)\n",
    "\n",
    "    # Merge all tables\n",
    "    rs = total.merge(n_partner, how='left', left_on=['country', 'product'], right_index=True).merge(\n",
    "        skewness, how='left', left_on=['country', 'product'], right_index=True).merge(\n",
    "        tw, how='left', on=['country', 'product']).merge(\n",
    "        first, how='left', on=['country', 'product']).rename(columns={'partner': '1_name'}).merge(\n",
    "        second, how='left', on=['country', 'product']).rename(columns={'partner': '2_name'}).merge(\n",
    "        third, how='left', on=['country', 'product']).rename(columns={'partner': '3_name'}).merge(\n",
    "        desc, how='left', on='product').iloc[:, [0, 1, -3, -2, -1] + list(range(2, 32))]\n",
    "    # Replace en country names with zh names\n",
    "    rs['country'].replace(ctry_map, inplace=True)\n",
    "    rs['1_name'].replace(ctry_map, inplace=True)\n",
    "    rs['2_name'].replace(ctry_map, inplace=True)\n",
    "    rs['3_name'].replace(ctry_map, inplace=True)\n",
    "\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 12min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_map = map(lambda f: aggr_data(f), files)\n",
    "df = reduce(lambda x, y: pd.concat([x, y], axis=0, ignore_index=True), df_map)\n",
    "\n",
    "# Output results\n",
    "df.to_csv('comp_aggregate_{}.csv'.format(aggr_level), sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
